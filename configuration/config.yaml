device: "cuda"
data:
  video_path: "../../mronconi/ego4d_data/v2/full_scale/"
  annotation_path: "../../mronconi/ego4d_data/v2/annotations/nlq_train.json"
  video_dataset: "/cluster/project/cvg/students/mronconi/ego4d_data/v2/internvideo1b_tag_clip_large_patch14/merged_10_video.pkl"
indexing:
  scene_length: 20.0
  video:
    model_name: "internvideo2-1b" # Options: "xclip", "internvideo2-1b", "internvideo2-6b"
    xclip_id: "microsoft/xclip-base-patch16"
    llava_id: "llava-hf/LLaVA-NeXT-Video-7B-hf"
    internvideo2_1b_id: "OpenGVLab/InternVideo2-Stage2-1B-224p-f8"
    internvideo2_6b_id: "OpenGVLab/InternVideo2-Stage2_6B"
    clip_id: "openai/clip-vit-base-patch32" # For XCLIP clustering only
  audio:
    audio_sample_rate: 48000
    asr_sample_rate: 16000
    asr_model_id: "openai/whisper-large-v3" # Upgraded from whisper-base
    audio_model_id: "laion/clap-htsat-unfused"
    audio_language: "en" # for transcription
    use_faster_whisper: true # Use faster-whisper for speed (requires: pip install faster-whisper)
    use_audio_events: true # Enable audio event detection
    use_diarization: false # Enable speaker diarization (requires: pip install pyannote.audio)
    audio_event_model_id: "MIT/ast-finetuned-audioset-10-10-0.4593" # Audio Spectrogram Transformer for event detection
    diarization_model_id: "pyannote/speaker-diarization-3.1" # Speaker diarization model
  text:
    text_model_id: "google/embeddinggemma-300m"
    llm_model_id: "Qwen/Qwen2.5-3B-Instruct" # LLM for screenplay generation
  caption:
    caption_model_id: "llava-hf/LLaVA-NeXT-Video-7B-hf"
  tag:
    vision_model_id: "openai/clip-vit-large-patch14" # CLIP for VisionTagger (visual tag extraction), openai/clip-vit-large-patch14, openai/clip-vit-base-patch32
    tagger_model_id: "Qwen/Qwen2.5-3B-Instruct" # LLM for text-based tagging (not used for videos)
    use_tagging: true
  # Window configuration for intermediate granularity between video and scenes
  use_windows: true
  window_size: 5 # Number of scenes per window
  window_stride: 5 # Number of scenes to slide between windows
retrieval:
  text_model_id: "google/embeddinggemma-300m"
  video_model_name: "internvideo2-1b" # "internvideo2-6b" # Options: "xclip", "internvideo2-1b", "internvideo2-6b"
  internvideo2_1b_id: "OpenGVLab/InternVideo2-Stage2-1B-224p-f8"
  internvideo2_6b_id: "OpenGVLab/InternVideo2-Stage2_6B" # Options: "OpenGVLab/InternVideo2-Stage1-1B-224p-f8", "OpenGVLab/InternVideo2-Stage2_6B"
  audio_model_id: "laion/clap-htsat-unfused"
  caption_model_id: "google/embeddinggemma-300m"
  rewriter_model_id: "Qwen/Qwen2.5-7B-Instruct"
  top_k_videos: 3
  top_k_windows: 2 # Number of windows to retrieve per video (only used if use_windows=true)
  top_k_scenes: 10
  use_windows: true # Enable/disable window-level retrieval
  modalities: ["text", "video"] # Run experiment uses all combinations
  fusion: "rrf"
  skip_video_retrieval: true # Skip video-level retrieval and only do scene-level retrieval using the correct video
  scene_merger:
    enabled: false # Enable/disable scene merging
    max_gap: 1.0 # Maximum time gap (seconds) between scenes to consider them consecutive
    min_scenes_to_merge: 2 # Minimum number of scenes required to create a merge
    max_scenes_to_merge: 5 # Maximum number of scenes to merge into one
    score_aggregation: "max" # How to aggregate scores: "max", "mean", or "sum"

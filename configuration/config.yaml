device: "cuda"
data:
  video_path: "../../tnanni/ego4d_data/v2/full_scale/"
  annotation_path: "../../tnanni/ego4d_data/v2/annotations/nlq_train.json"
  video_dataset: "/cluster/project/cvg/students/tnanni/ego4d_data/v2/redone_internvideo_encoded/5d8dfcfe-93e4-465e-a548-44c1442ffa29_encoded.pkl"
indexing:
  video:
    model_name: "internvideo2" # Options: "xclip", "internvideo2"
    xclip_id: "microsoft/xclip-base-patch16"
    llava_id: "llava-hf/LLaVA-NeXT-Video-7B-hf"
    internvideo2_id: "OpenGVLab/InternVideo2-Stage2-1B-224p-f8"
    clip_id: "openai/clip-vit-base-patch32" # For XCLIP clustering only
  audio:
    audio_sample_rate: 48000
    asr_sample_rate: 16000
    asr_model_id: "openai/whisper-large-v3" # Upgraded from whisper-base
    audio_model_id: "laion/clap-htsat-unfused"
    audio_language: "en" # for transcription
    use_faster_whisper: true # Use faster-whisper for speed (requires: pip install faster-whisper)
    use_audio_events: true # Enable audio event detection
    use_diarization: false # Enable speaker diarization (requires: pip install pyannote.audio)
    audio_event_model_id: "MIT/ast-finetuned-audioset-10-10-0.4593" # Audio Spectrogram Transformer for event detection
    diarization_model_id: "pyannote/speaker-diarization-3.1" # Speaker diarization model
  text:
    text_model_id: "google/embeddinggemma-300m"
    llm_model_id: "Qwen/Qwen2.5-3B-Instruct" # LLM for screenplay generation
  caption:
    caption_model_id: "llava-hf/LLaVA-NeXT-Video-7B-hf"
  tag:
    tagger_model_id: "Qwen/Qwen2.5-3B-Instruct" # LLM for tagging
  # Window configuration for intermediate granularity between video and scenes
  window_size: 12 # Number of scenes per window
  window_stride: 4 # Number of scenes to slide between windows
retrieval:
  text_model_id: "google/embeddinggemma-300m"
  video_model_id: "OpenGVLab/InternVideo2-Stage2-1B-224p-f8" #"OpenGVLab/InternVideo2-Stage1-1B-224p-f8"  # Options: "OpenGVLab/InternVideo2-Stage1-1B-224p-f8"
  audio_model_id: "laion/clap-htsat-unfused"
  caption_model_id: "google/embeddinggemma-300m"
  rewriter_model_id: "Qwen/Qwen2.5-7B-Instruct"
  top_k_videos: 3
  top_k_windows: 10 # Number of windows to retrieve per video (only used if use_windows=true)
  top_k_scenes: 10
  use_windows: true # Enable/disable window-level retrieval
  modalities: ["text"]
  fusion: "rrf"
  skip_video_retrieval: true
  scene_merger:
    enabled: false # Enable/disable scene merging
    max_gap: 1.0 # Maximum time gap (seconds) between scenes to consider them consecutive
    min_scenes_to_merge: 2 # Minimum number of scenes required to create a merge
    max_scenes_to_merge: 5 # Maximum number of scenes to merge into one
    score_aggregation: "max" # How to aggregate scores: "max", "mean", or "sum"

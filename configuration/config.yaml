device: "cuda"
data:
  video_path: "../../tnanni/ego4d_data/v2/full_scale/"
  annotation_path: "../../tnanni/ego4d_data/v2/annotations/nlq_train.json"
  video_dataset: "/cluster/project/cvg/students/tnanni/ego4d_data/v2/internvideo1b_window_5s_encoded/merged_window_5s_videos.pkl"
indexing:
  scene_length: 12.0
  video:
    model_name: "internvideo2-6b" # Options: "xclip", "internvideo2-1b", "internvideo2-6b"
    xclip_id: "microsoft/xclip-base-patch16"
    llava_id: "llava-hf/LLaVA-NeXT-Video-7B-hf"
    internvideo2_1b_id: "OpenGVLab/InternVideo2-Stage2-1B-224p-f8"
    internvideo2_6b_id: "OpenGVLab/InternVideo2-Stage2_6B"
    clip_id: "openai/clip-vit-base-patch32" # For XCLIP clustering only
  audio:
    audio_sample_rate: 48000
    asr_sample_rate: 16000
    asr_model_id: "openai/whisper-large-v3" # Upgraded from whisper-base
    audio_model_id: "laion/clap-htsat-unfused"
    audio_language: "en" # for transcription
    use_faster_whisper: true # Use faster-whisper for speed (requires: pip install faster-whisper)
    use_audio_events: true # Enable audio event detection
    use_diarization: false # Enable speaker diarization (requires: pip install pyannote.audio)
    audio_event_model_id: "MIT/ast-finetuned-audioset-10-10-0.4593" # Audio Spectrogram Transformer for event detection
    diarization_model_id: "pyannote/speaker-diarization-3.1" # Speaker diarization model
  text:
    text_model_id: "google/embeddinggemma-300m"
    llm_model_id: "Qwen/Qwen2.5-3B-Instruct" # LLM for screenplay generation
  caption:
    caption_model_id: "llava-hf/LLaVA-NeXT-Video-7B-hf"
  tag:
    tagger_model_id: "Qwen/Qwen2.5-3B-Instruct" # LLM for tagging
  # Window configuration for intermediate granularity between video and scenes
  use_windows: true
  window_size: 5 # Number of scenes per window
  window_stride: 5 # Number of scenes to slide between windows
retrieval:
  text_model_id: "google/embeddinggemma-300m"
  video_model_name: "internvideo2-6b" # Options: "xclip", "internvideo2-1b", "internvideo2-6b"
  internvideo2_1b_id: "OpenGVLab/InternVideo2-Stage2-1B-224p-f8"
  internvideo2_6b_id: "OpenGVLab/InternVideo2-Stage2_6B" # Options: "OpenGVLab/InternVideo2-Stage1-1B-224p-f8", "OpenGVLab/InternVideo2-Stage2_6B"
  audio_model_id: "laion/clap-htsat-unfused"
  caption_model_id: "google/embeddinggemma-300m"
  rewriter_model_id: "Qwen/Qwen2.5-7B-Instruct"
  top_k_videos: 3
  top_k_windows: 5 # Number of windows to retrieve per video (only used if use_windows=true)
  top_k_scenes: 30
  use_windows: true # Enable/disable window-level retrieval
  modalities: ["text"] # Run experiment uses all combinations
  fusion: "rrf"
  skip_video_retrieval: true # Skip video-level retrieval and only do scene-level retrieval using the correct video
  scene_merger:
    enabled: false # Enable/disable scene merging
    max_gap: 1.0 # Maximum time gap (seconds) between scenes to consider them consecutive
    min_scenes_to_merge: 2 # Minimum number of scenes required to create a merge
    max_scenes_to_merge: 5 # Maximum number of scenes to merge into one
    score_aggregation: "max" # How to aggregate scores: "max", "mean", or "sum"
  # Multi-resolution retrieval: Search at multiple scene granularities
  multi_resolution:
    enabled: true # Enable multi-resolution scene retrieval
    # Multipliers for creating virtual scenes from base resolution
    # e.g., if scene_length=12.0, multipliers [2.0, 3.0] create 24s and 36s virtual scenes
    resolution_multipliers: [2.0, 3.0]
    overlap_factor: 0.5 # Overlap between virtual scenes (0.5 = 50% overlap)
    fusion_strategy: "rrf" # How to fuse results: "rrf", "max", or "weighted"
    # Resolution weights (only used if fusion_strategy="weighted")
    resolution_weights:
      12.0: 1.0  # Base resolution weight
      24.0: 0.8  # 2x resolution weight
      36.0: 0.6  # 3x resolution weight

device: "cuda"
data:
  video_path: "../ego4d_data/v2/full_scale/"
  annotation_path: "../ego4d_data/v2/annotations/nlq_train.json"
  video_dataset: "../ego4d_data/v2/llava_encoded_videos/llava_10_videos.pkl"
indexing:
  video:
    model_name: "qwen2-vl" # Options: "xclip", "qwen2-vl"
    xclip_id: "microsoft/xclip-base-patch16"
    qwen2_vl_id: "Qwen/Qwen2-VL-7B-Instruct"
    clip_id: "openai/clip-vit-base-patch32"  # For XCLIP clustering only
  audio:
    audio_sample_rate: 48000
    asr_sample_rate: 16000
    asr_model_id: "openai/whisper-large-v3" # Upgraded from whisper-base
    audio_model_id: "laion/clap-htsat-unfused"
    audio_language: "en" # for transcription
    use_faster_whisper: true # Use faster-whisper for speed (requires: pip install faster-whisper)
    use_audio_events: true # Enable audio event detection
    use_diarization: false # Enable speaker diarization (requires: pip install pyannote.audio)
    audio_event_model_id: "MIT/ast-finetuned-audioset-10-10-0.4593" # Audio Spectrogram Transformer for event detection
    diarization_model_id: "pyannote/speaker-diarization-3.1" # Speaker diarization model
  text:
    text_model_id: "all-MiniLM-L6-v2"
  caption:
    use_captioner: "captioner2" # Options: "captioner1" (BLIP), "captioner2" (Qwen2-VL)
    caption_model_id: "Salesforce/blip-image-captioning-base"
    caption2_model_id: "Qwen/Qwen2-VL-7B-Instruct"
retrieval:
  text_model_id: "all-MiniLM-L6-v2"
  video_model_id: "laion/CLIP-ViT-H-14-laion2B-s32B-b79K"
  audio_model_id: "laion/clap-htsat-unfused"
  caption_model_id: "all-MiniLM-L6-v2"
  rewriter_model_id: "Qwen/Qwen2.5-7B-Instruct"
  top_k_videos: 3
  top_k_scenes: 10
  modalities: ["video", "text"]
  fusion: "rrf"
